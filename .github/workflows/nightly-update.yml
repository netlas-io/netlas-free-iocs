name: Run Update Daily

on:
  schedule:
    - cron: "0 1 * * *" # Runs daily at 01:00 UTC

jobs:
  run-script:
    runs-on: ubuntu-latest

    steps:
      # Step 1: Check out the repository
      - name: Checkout repository
        uses: actions/checkout@v3

      # Step 2: Install s3cmd
      - name: Install s3cmd and yq
        run: |
          sudo apt-get update
          sudo apt-get install -y s3cmd yq wget

      # Step 3: Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      # Step 4: Install dependencies
      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      # Step 5: Extract database_file using helper script
      - name: Get database_file from config.yaml
        id: get-database-file
        run: |
          DATABASE_FILE=$(yq '.database_file' config.yaml)
          if [ -z "$DATABASE_FILE" ]; then
            echo "Error: database_file not found in config.yaml"
            exit 1
          fi
          echo "DATABASE_FILE=$DATABASE_FILE" >> $GITHUB_ENV
        env:
          DATABASE_FILE: ""

      # Step 6: Download database_file and checksum file from S3
      - name: Config S3
        run: |
          echo "[default]" > ~/.s3cfg
          echo "access_key = $CF_R2_PUBLIC_BUCKET_ACCESS_KEY" >> ~/.s3cfg
          echo "secret_key = $CF_R2_PUBLIC_BUCKET_SECRET_KEY" >> ~/.s3cfg
          echo "host_base = $CF_ACCOUNT_ID.r2.cloudflarestorage.com" >> ~/.s3cfg
          echo "host_bucket = %(bucket).$CF_ACCOUNT_ID.r2.cloudflarestorage.com" >> ~/.s3cfg
          if s3cmd ls s3://public/; then
            echo "Bucket is available."
          else
            echo "Error: Bucket is not available."
          exit 1
          fi
        env:
          CF_R2_PUBLIC_BUCKET_ACCESS_KEY: ${{ secrets.CF_R2_PUBLIC_BUCKET_ACCESS_KEY }}
          CF_R2_PUBLIC_BUCKET_SECRET_KEY: ${{ secrets.CF_R2_PUBLIC_BUCKET_SECRET_KEY }}
          CF_ACCOUNT_ID: ${{ secrets.CF_ACCOUNT_ID }}
          DATABASE_FILE: ${{ env.DATABASE_FILE }}

      # Step 7: Run the script
      - name: Run the script
        run: bash update.sh
        env:
          NETLAS_API_KEY: ${{ secrets.NETLAS_API_KEY }}
          DATABASE_FILE: ${{ env.DATABASE_FILE }}

      # Step 8: Check integrity and upload if necessary
      - name: Verify checksum and upload to S3 if changed
        run: |
          CHECKSUM_MISMATCH=0
          if [ -f "$DATABASE_FILE.md5" ]; then
            md5sum -c $DATABASE_FILE.md5 --quiet || CHECKSUM_MISMATCH=1

            if [ $CHECKSUM_MISMATCH -eq 1 ]; then
              echo "Checksum mismatch detected. Updating checksum and uploading to S3."
              md5sum $DATABASE_FILE > $DATABASE_FILE.md5
              s3cmd put $DATABASE_FILE s3://public/$DATABASE_FILE
              s3cmd put $DATABASE_FILE.md5 s3://public/$DATABASE_FILE.md5
            else
              echo "Checksum verification passed. No updates detected."
            fi
          else
            echo "Checksum file not found. Skipping verification."
          fi
        env:
          CF_R2_PUBLIC_BUCKET_ACCESS_KEY: ${{ secrets.CF_R2_PUBLIC_BUCKET_ACCESS_KEY }}
          CF_R2_PUBLIC_BUCKET_SECRET_KEY: ${{ secrets.CF_R2_PUBLIC_BUCKET_SECRET_KEY }}
          CF_ACCOUNT_ID: ${{ secrets.CF_ACCOUNT_ID }}
          DATABASE_FILE: ${{ env.DATABASE_FILE }}

      # Step 9: Remove database_file and checksum before commit
      - name: Clean up temporary files
        run: |
          rm -f $DATABASE_FILE $DATABASE_FILE.md5
        env:
          DATABASE_FILE: ${{ env.DATABASE_FILE }}

      # Step 10: Commit and push the changes back to the repository
      - name: Commit and push changes
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          git add .
          git commit -m "Update results from script run on $(date)"
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}